---
layout: post
title: "Researchers describe how to tell if ChatGPT is confabulating"
date: 2024-06-22
---

> The AI could have been trained on misinformation; the answer could
require some extrapolation from facts that the LLM isn't capable of; or
some aspect of the LLM's training might have incentivized a falsehood. Now,
researchers from the University of Oxford say they've found a relatively
simple way to determine when LLMs appear to be confabulating that works
with all popular models and across a broad range of subjects. To use an
example from the researchers' paper, "Paris," "It's in Paris," and
"France's capital, Paris" are all valid answers to "Where's the Eiffel
Tower?" So, statistical uncertainty, termed entropy in this context, can
arise either when the LLM isn't certain about how to phrase the right
answer or when it can't identify the right answer.

Source: [Researchers describe how to tell if ChatGPT is confabulating](
https://arstechnica.com/ai/2024/06/researchers-describe-how-to-tell-if-chatgpt-is-confabulating/
)

Sounds like they might come up with something

